{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecee8af",
   "metadata": {},
   "source": [
    "# Trustcall\n",
    "\n",
    "- Memory management tool calling, built using langgraph\n",
    "- Updates schemas without losing contents.\n",
    "- LLM powered JSON patching - taking leverage from LLM's structured output mastery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56fed7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-2.5-pro',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9dc4820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, Annotated\n",
    "\n",
    "from trustcall import create_extractor\n",
    "\n",
    "Interests = Literal['programming', 'ai', 'art', 'lifestyle', 'finance']\n",
    "\n",
    "class Publication(BaseModel):\n",
    "    \"\"\"Publication - Defining the Article\"\"\"\n",
    "    id: str = Field(description=\"Id of the publication\")\n",
    "    title: str = Field(description='Title of the Publication')\n",
    "    summary: str = Field(description='Summary of the article in about 100 words')\n",
    "    author: str = Field(description='Author Full name')\n",
    "    url: str = Field(description=\"Link for the publication\")\n",
    "\n",
    "class User(BaseModel):\n",
    "    interests: list[Interests] | None = Field(description=\"List/Array of interests: Example ['science']\", default=None)\n",
    "    reads: list[Publication] | None = Field(description='list/array of publications user reads', default=None)\n",
    "    recommended_reads: list[Publication] | None = Field(description=\"Recommended list of Publications based on interests\", default=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e76b9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "USERS: dict[str, User] = {\n",
    "    'alice': User(interests=['programming', 'ai']),\n",
    "    'bob': User(interests=['finance'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7c278cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alice': User(interests=['programming', 'ai'], reads=None, recommended_reads=None),\n",
       " 'bob': User(interests=['finance'], reads=None, recommended_reads=None)}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a3e2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "existing_user = USERS['alice']\n",
    "users_ns: tuple[Literal['users'], str] = ('users', 'alice')\n",
    "user_id = str(uuid4())\n",
    "in_memory_store.put(\n",
    "    users_ns,\n",
    "    user_id,\n",
    "    existing_user.model_dump()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d73491ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, Response\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_posts(category: str = None, size: int = 3) -> list[Publication]:\n",
    "    \"\"\"Get posts from API based on category\"\"\"\n",
    "    if category is None:\n",
    "        return Publication(\n",
    "            title='Welcome post; Know more about lifestyle, ai, programming, art, finance',\n",
    "            id='lifestyle-999',\n",
    "            summary='Sample summary',\n",
    "            author='Alice',\n",
    "            url='dummy.com'\n",
    "        )\n",
    "\n",
    "    publications: list[Publication] = get(f\"\"\"http://localhost:4001/feed?category={category}\"\"\", headers = {'Cache-Control': 'no-cache'}).json()\n",
    "    return publications[:size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "79c948ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/6bcjxw75117_6m_84gvdp5080000gn/T/ipykernel_97951/3770273754.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res: list[Publication] = get_posts(category='ai')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseTool.__call__() got an unexpected keyword argument 'category'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res: \u001b[38;5;28mlist\u001b[39m[Publication] = \u001b[43mget_posts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mai\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code-machine/Gen AI/langgraph/lc-academy-env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: BaseTool.__call__() got an unexpected keyword argument 'category'"
     ]
    }
   ],
   "source": [
    "res: list[Publication] = get_posts(category='ai')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "485333ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_ns = ('recommendations', 'ai')\n",
    "in_memory_store.put(\n",
    "    recommendations_ns,\n",
    "    user_id,\n",
    "    {\n",
    "        'interests': existing_user.interests,\n",
    "        'recommended_reads': res \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d99cfe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['users', 'alice'], key='8e1eaa1a-788f-4b16-8ea5-a0db99d4f368', value={'interests': ['programming', 'ai'], 'reads': None, 'recommended_reads': None}, created_at='2025-08-04T02:50:52.501450+00:00', updated_at='2025-08-04T02:50:52.501453+00:00', score=None)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories = in_memory_store.search(users_ns)\n",
    "memories # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2bc4a5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['recommendations', 'ai'], key='8e1eaa1a-788f-4b16-8ea5-a0db99d4f368', value={'interests': ['programming', 'ai'], 'recommended_reads': [{'id': 'ai-301', 'title': 'Fine-Tuning LLMs: What You Need to Know', 'author': 'Dr. Emily Zhou', 'summary': 'Key considerations and best practices for LLM fine-tuning in 2025.', 'url': 'https://medium.com/ai/fine-tuning-llms-efghi'}, {'id': 'ai-302', 'title': 'The Rise of Responsible AI', 'author': 'Martin Lewis', 'summary': 'Balancing innovation and ethics in artificial intelligence.', 'url': 'https://medium.com/ai/rise-of-responsible-ai-fghij'}, {'id': 'ai-303', 'title': 'AI Agents: Hype, Hope, and Reality', 'author': 'Cindy Alvarez', 'summary': 'Parsing fact from fiction in the world of autonomous AI agents.', 'url': 'https://medium.com/ai/ai-agents-hype-vs-reality-plokm'}]}, created_at='2025-08-04T02:54:37.733332+00:00', updated_at='2025-08-04T02:54:37.733334+00:00', score=None)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubs = in_memory_store.search(recommendations_ns)\n",
    "pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0deadd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': ['users', 'alice'],\n",
       " 'key': '8e1eaa1a-788f-4b16-8ea5-a0db99d4f368',\n",
       " 'value': {'interests': ['programming', 'ai'],\n",
       "  'reads': None,\n",
       "  'recommended_reads': None},\n",
       " 'created_at': '2025-08-04T02:50:52.501450+00:00',\n",
       " 'updated_at': '2025-08-04T02:50:52.501453+00:00',\n",
       " 'score': None}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "61b59e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROFILE_INSTRUCTION = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses. Note: From the conversation, try identifying the category with the known interests\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "CREATE_OR_UPDATE_USER_PROFILE_INSTRUCTION = \"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "<profile>\n",
    "{profile}\n",
    "</profile>\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "    - Personal details (name, location)\n",
    "    - Preferences (likes, dislikes)\n",
    "    - Interests and Reads\n",
    "    - Past experiences\n",
    "    - Goals or future plans\n",
    "    - Keep adding attributes to the profile as you discover things\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: \n",
    "1. The goal is to create/update user profile information and not to answer the questions based on chat history.\n",
    "2. Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Example Output: \"**User Information:**\\n- User's name is Bob.\\n- Likes to read about Programming.\"\n",
    "\n",
    "please update the user information based on the chat history:\n",
    "\"\"\"\n",
    "\n",
    "CREATE_OR_UPDATE_RECOMMENDATIONS_INSTRUCTION = \"\"\"You are a helpful recommendation assistant for recommending publications based on user interests.\n",
    "\n",
    "Following is the list of current user interests (maybe empty as well).\n",
    "\n",
    "CURRENT USER INTERESTS: {categories}\n",
    "\n",
    "CURRENT RECOMMENDED READS: {rec_reads}\n",
    "\n",
    "Steps to do:\n",
    "1. If its a new user, meaning interests are empty get 1 recommendation for all five categories, so we get to know the users interests\n",
    "2. Get three publications using the `get_posts` tool with `category` as the one user is interested in.\n",
    "3. Send an updated list of `recommended_reads` for the user based on the interests and previous reads if any.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db92334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I just finished reading Gen AI applications\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4c326804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[User(interests=None, reads=[Publication(id='001', title='Gen AI applications', summary='', author='', url='')], recommended_reads=None)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4')\n",
    "user_profile_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[User, Publication],\n",
    "    tool_choice=\"User\",\n",
    "    enable_inserts=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "user_profile = user_profile_extractor.invoke({\n",
    "    \"messages\": [SystemMessage(content=USER_PROFILE_INSTRUCTION.format(memory=memories[0].dict()['value']))] + conversation,\n",
    "    # \"existing\": memories[0].dict()['value']  # Optionally provide current todos for update\n",
    "})\n",
    "\n",
    "user_profile['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1db7d77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Publication(id='1', title='Clean Code', summary='A handbook of Agile software craftsmanship', author='Robert C. Martin', url='')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[User, Publication],\n",
    "    tool_choice=\"Publication\",\n",
    "    enable_inserts=True\n",
    ")\n",
    "\n",
    "publications= publications_extractor.invoke({\n",
    "    \"messages\": [SystemMessage(content=CREATE_OR_UPDATE_USER_PROFILE_INSTRUCTION.format(profile=memories[0].dict()['value']))] + conversation,\n",
    "    # \"existing\": memories[0].dict()['value']  # Optionally provide current todos for update\n",
    "})\n",
    "\n",
    "publications['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "95d47db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interests': ['programming', 'ai'],\n",
       " 'recommended_reads': [{'id': 'ai-301',\n",
       "   'title': 'Fine-Tuning LLMs: What You Need to Know',\n",
       "   'author': 'Dr. Emily Zhou',\n",
       "   'summary': 'Key considerations and best practices for LLM fine-tuning in 2025.',\n",
       "   'url': 'https://medium.com/ai/fine-tuning-llms-efghi'},\n",
       "  {'id': 'ai-302',\n",
       "   'title': 'The Rise of Responsible AI',\n",
       "   'author': 'Martin Lewis',\n",
       "   'summary': 'Balancing innovation and ethics in artificial intelligence.',\n",
       "   'url': 'https://medium.com/ai/rise-of-responsible-ai-fghij'},\n",
       "  {'id': 'ai-303',\n",
       "   'title': 'AI Agents: Hype, Hope, and Reality',\n",
       "   'author': 'Cindy Alvarez',\n",
       "   'summary': 'Parsing fact from fiction in the world of autonomous AI agents.',\n",
       "   'url': 'https://medium.com/ai/ai-agents-hype-vs-reality-plokm'}]}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_mem = in_memory_store.search(recommendations_ns)\n",
    "rec_mem[0].dict()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0b7c1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spy:\n",
    "    def __init__(self):\n",
    "        self.called_tools = []\n",
    "\n",
    "    def __call__(self, run):\n",
    "        # Collect information about the tool calls made by the extractor.\n",
    "        q = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type == \"chat_model\":\n",
    "                self.called_tools.append(\n",
    "                    r.outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n",
    "                )\n",
    "\n",
    "# Initialize the spy\n",
    "spy = Spy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ed95dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_info(tool_calls, schema_name):\n",
    "    \"\"\"Extract information from tool calls for both patches and new memories.\n",
    "    \n",
    "    Args:\n",
    "        tool_calls: List of tool calls from the model\n",
    "        schema_name: Name of the schema tool (e.g., \"User\", \"Publication\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize list of changes\n",
    "    changes = []\n",
    "    \n",
    "    for call_group in tool_calls:\n",
    "        for call in call_group:\n",
    "            if call['name'] == 'PatchDoc':\n",
    "                changes.append({\n",
    "                    'type': 'update',\n",
    "                    'doc_id': call['args']['json_doc_id'],\n",
    "                    'planned_edits': call['args']['planned_edits'],\n",
    "                    'value': call['args']['patches'][0]['value']\n",
    "                })\n",
    "            elif call['name'] == schema_name:\n",
    "                changes.append({\n",
    "                    'type': 'new',\n",
    "                    'value': call['args']\n",
    "                })\n",
    "\n",
    "    # Format results as a single string\n",
    "    result_parts = []\n",
    "    for change in changes:\n",
    "        if change['type'] == 'update':\n",
    "            result_parts.append(\n",
    "                f\"Document {change['doc_id']} updated:\\n\"\n",
    "                f\"Plan: {change['planned_edits']}\\n\"\n",
    "                f\"Added content: {change['value']}\"\n",
    "            )\n",
    "        else:\n",
    "            result_parts.append(\n",
    "                \n",
    "                f\"New {schema_name} created:\\n\"\n",
    "                f\"Content: {change['value']}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(result_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "27e10f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'tool', 'content': '', 'tool_call_id': 'call_AAa8blCpCWKH8zFXGZdBu0aL'}]}\n"
     ]
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools([get_posts])\n",
    "\n",
    "\n",
    "recommendations_extractor = create_extractor(\n",
    "    llm_with_tools,\n",
    "    tools=[User, get_posts],\n",
    "    # tool_choice='get_posts',\n",
    "    enable_inserts=True\n",
    ")\n",
    "\n",
    "publications= publications_extractor.invoke({\n",
    "    \"messages\": [SystemMessage(\n",
    "        content=CREATE_OR_UPDATE_RECOMMENDATIONS_INSTRUCTION.format(\n",
    "            categories=memories[0].dict()['value'],\n",
    "            rec_reads=rec_mem[0].dict()['value']['recommended_reads']\n",
    "            )\n",
    "        )] + conversation,\n",
    "    # \"existing\": rec_mem[0].dict()['value']['recommended_reads']  # Optionally provide current todos for update\n",
    "})\n",
    "\n",
    "tool_calls = publications['messages'][-1].tool_calls\n",
    "\n",
    "    # Extract the changes made by Trustcall and add the the ToolMessage returned to task_mAIstro\n",
    "todo_update_msg = extract_tool_info(spy.called_tools, 'get_tools')\n",
    "print({\"messages\": [{\"role\": \"tool\", \"content\": todo_update_msg, \"tool_call_id\":tool_calls[0]['id']}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b83585b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_AAa8blCpCWKH8zFXGZdBu0aL', 'function': {'arguments': '{\\n  \"id\": \"ai-304\",\\n  \"title\": \"Gen AI applications\",\\n  \"summary\": \"Exploring the emerging applications of gen AI.\",\\n  \"author\": \"Sam K. Thompson\",\\n  \"url\": \"https://medium.com/ai/gen-ai-applications\"\\n}', 'name': 'Publication'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 591, 'total_tokens': 652, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-C0g45DP4ZwqZavTVySQ5PeGZp6kbN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6c01d955-978c-47e3-a590-b02a1208832d-0', tool_calls=[{'name': 'Publication', 'args': {'id': 'ai-304', 'title': 'Gen AI applications', 'summary': 'Exploring the emerging applications of gen AI.', 'author': 'Sam K. Thompson', 'url': 'https://medium.com/ai/gen-ai-applications'}, 'id': 'call_AAa8blCpCWKH8zFXGZdBu0aL', 'type': 'tool_call'}], usage_metadata={'input_tokens': 591, 'output_tokens': 61, 'total_tokens': 652, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'responses': [Publication(id='ai-304', title='Gen AI applications', summary='Exploring the emerging applications of gen AI.', author='Sam K. Thompson', url='https://medium.com/ai/gen-ai-applications')],\n",
       " 'response_metadata': [{'id': 'call_AAa8blCpCWKH8zFXGZdBu0aL'}],\n",
       " 'attempts': 1}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
